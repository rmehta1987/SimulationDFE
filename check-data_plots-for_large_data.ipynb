{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi, SNLE, MNLE, SNRE, SNRE_A\n",
    "from sbi.utils.posterior_ensemble import NeuralPosteriorEnsemble\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils import MultipleIndependent\n",
    "from sbi.neural_nets.embedding_nets import PermutationInvariantEmbedding, FCEmbedding\n",
    "from sbi.utils.user_input_checks import process_prior, process_simulator\n",
    "from sbi.utils import get_density_thresholder, RestrictedPrior\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "import numpy as np\n",
    "import moments\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import atexit\n",
    "import torch.nn.functional as F\n",
    "import subprocess\n",
    "import sparselinear as sl\n",
    "from sortedcontainers import SortedDict\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "import re\n",
    "from monarch_linear import MonarchLinear\n",
    "import pdb\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR) # See: https://github.com/matplotlib/matplotlib/issues/14523\n",
    "from collections import defaultdict\n",
    "from sbi.analysis import pairplot\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=55855\n",
    "the_device='cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryNet(nn.Module):\n",
    "    def __init__(self, sample_size, block_sizes, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.sample_size = sample_size # For monarch this needs to be divisible by the block size\n",
    "        self.block_size = block_sizes\n",
    "        self.linear4 = MonarchLinear(sample_size, int(sample_size / 10), nblocks=self.block_size[0]) # 11171\n",
    "        self.linear5 = MonarchLinear(int(self.sample_size / 10), int(self.sample_size / 10) , nblocks=self.block_size[1]) # 11171\n",
    "        self.linear6 = MonarchLinear(int(self.sample_size / 10), int(self.sample_size / 10), nblocks=self.block_size[2]) # 11171\n",
    "\n",
    "        self.model = nn.Sequential(self.linear4, nn.Dropout(dropout_rate), nn.GELU(),\n",
    "                                   self.linear5, nn.Dropout(dropout_rate), nn.GELU(),\n",
    "                                   self.linear6) \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_datafrom_hdf5(path_to_sim_file: str):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        path_to_sim_file (str): _description_\n",
    "    \"\"\"    \n",
    "    #TODO probably will be better to use https://github.com/quantopian/warp_prism for faster look-up tables\n",
    "    global loaded_file \n",
    "    global loaded_file_keys\n",
    "    global loaded_tree\n",
    "    import h5py\n",
    "    loaded_file = h5py.File(path_to_sim_file, 'r')\n",
    "    loaded_file_keys = list(loaded_file.keys())\n",
    "    loaded_tree = KDTree(np.asarray(loaded_file_keys)[:,None]) # needs to have a column dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_generate_sim_data(prior: float) -> torch.float32:\n",
    "\n",
    "    data = np.zeros((sample_size*2-1))\n",
    "    theprior = prior\n",
    "    gammas = -1*10**(theprior.cpu().numpy().squeeze())\n",
    "    #theprior=prior\n",
    "    #mis_id = prior[-1].cpu().numpy()\n",
    "    mis_id=0\n",
    "    for a_prior in gammas:\n",
    "        _, idx = loaded_tree.query(a_prior, k=(1,)) # the k sets number of neighbors, while we only want 1, we need to make sure it returns an array that can be indexed\n",
    "        fs = loaded_file[loaded_file_keys[idx[0]]][:]\n",
    "        #fs = fs*.01552243512  # scale to lof theta\n",
    "        fs = fs*.2077370846  # scale to missense theta\n",
    "        fs = (1 - mis_id)*fs + mis_id * fs[::-1]\n",
    "        data += fs \n",
    "    data /= theprior.shape[0]\n",
    "    return torch.exp(torch.log(torch.nn.functional.relu(torch.tensor(data, device=the_device)+1)).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_true_data(a_path: str, type: int) -> torch.float32:\n",
    "    \"\"\"Loads a true SFS, note that the sample size must be consistent with the passed parameters\n",
    "\n",
    "    Args:\n",
    "        path (str): Where the true-SFS is located, must be a numpy array\n",
    "        type (int): is data stored in numpy pickle (0) or torch pickle (1)\n",
    "\n",
    "    Returns:\n",
    "        Returns the SFS of the true data-set\n",
    "    \"\"\"\n",
    "    if type == 0:\n",
    "        sfs = np.load(a_path)\n",
    "        sfs = torch.tensor(sfs, device=the_device).type(torch.float32)\n",
    "    else:\n",
    "        sfs = torch.load(a_path)\n",
    "        sfs.to(the_device)\n",
    "    assert sfs.shape[0] == sample_size*2-1, \"Sample Size must be the same dimensions as the Site Frequency Spectrum, SFS shape: {} and sample shape (2*N-1): {}\".format(sfs.shape[0], sample_size*2-1)\n",
    "\n",
    "    return sfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sim_datafrom_hdf5('chr10_sim_genome_wide_mut_sfs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "udist = torch.distributions.uniform.Uniform(-6.0*torch.ones(50),4.0*torch.ones(50))\n",
    "uniform_samples = udist.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_posterior_lof=torch.load('nfe_restriction_classifier_lof_embedding_genome_wide_fixed_theta.pkl')\n",
    "last_posterior_missense=torch.load('nfe_restriction_classifier_mis_embedding_genome_wide.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_x_mis = (load_true_data('emperical_missense_sfs_nfe.npy', 0)[:-1]).unsqueeze(0)\n",
    "true_x_lof = (load_true_data('emperical_lof_sfs_nfe.npy', 0)[:-1]).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a posterior-like proposal\n",
    "accept_reject_fn = get_density_thresholder(last_posterior, quantile=1e-5, num_samples_to_estimate_support=100000)\n",
    "proposal = RestrictedPrior(last_posterior._prior, accept_reject_fn, last_posterior, sample_with=\"sir\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you using a classifier based proposal\n",
    "proposal_lof = last_posterior_lof.restrict_prior(allowed_false_negatives=0.0)\n",
    "proposal_mis = last_posterior_missense.restrict_prior(allowed_false_negatives=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples_lof = proposal_lof.sample((1000,), oversampling_factor=1024)\n",
    "obs_samples_mis = proposal_mis.sample((1000,), oversampling_factor=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sampled selection coefficients: torch.Size([1000, 60])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of sampled selection coefficients: {}\".format(obs_samples_mis.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:38<00:00, 25.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for obs_sample in tqdm(obs_samples):\n",
    "    fs = aggregated_generate_sim_data(obs_sample)\n",
    "    predicted_fs.append(fs.unsqueeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency spectrum containing all bins (1000, 111709)\n"
     ]
    }
   ],
   "source": [
    "new_predicted_fs = np.asarray(predicted_fs).squeeze(1)\n",
    "print(\"Shape of frequency spectrum containing all bins {}\".format(new_predicted_fs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 25):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    sns.histplot(np.log10(new_predicted_fs[:,i-1]))\n",
    "    plt.axvline(x=np.mean(np.log10(new_predicted_fs[:,i-1])), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(np.log10(new_predicted_fs[:,i-1])), color='k', label=\"median\")\n",
    "    ax.axline((np.log10(true_x[i-1]), 1), (np.log10(true_x[i-1]),100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(np.log10(true_x[i-1]),i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_mis_36_coefficients_round2_blur_2_hs.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 25):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    sns.histplot(np.log10(new_predicted_fs2[:,i-1]))\n",
    "    plt.axvline(x=np.mean(np.log10(new_predicted_fs2[:,i-1])), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(np.log10(new_predicted_fs2[:,i-1])), color='k', label=\"median\")\n",
    "    ax.axline((np.log10(true_x[i-1]), 1), (np.log10(true_x[i-1]),100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(np.log10(true_x[i-1]),i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_lof_36_coefficients_round2_blur_2_s.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without pmsid: [846931.7    197617.72    89259.19    47550.04    27361.467   16637.205\n",
      "  10606.516    7071.227    4928.155    3590.4414]\n",
      "tensor([[1410237.,  365768.,  168386.,  ...,       0.,       0.,       0.]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mean_predicted = np.mean(new_predicted_fs[:],axis=0)\n",
    "print(f\"Without pmsid: {mean_predicted[:10]}\")\n",
    "print(true_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency bins on x-axis for plotting 200\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0,mean_predicted[0:200].shape[0])\n",
    "print(\"Shape of frequency bins on x-axis for plotting {}\".format(x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=np.log10(mean_predicted[:200]+1), label=\"Predicted\")\n",
    "sns.scatterplot(x=x,y=np.log10(true_x.cpu().numpy()[0,:200]+1), label=\"Emperical\")\n",
    "plt.title(\"Mean of posterior predicted SFS vs True SFS\")\n",
    "plt.ylabel(\"Log Scaled Allele Frequency\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('ppc_scatter_mis_classifier_check.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=np.log10(mean_predicted2+1), label=\"Predicted\")\n",
    "sns.scatterplot(x=x,y=np.log10(true_x+1), label=\"Emperical\")\n",
    "plt.title(\"Mean of posterior predicted SFS vs True SFS\")\n",
    "plt.ylabel(\"Log Scaled Allele Frequency\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('ppc_scatter_mis_36_coefficients_round1.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print missense shape: torch.Size([20000, 60])\n",
      "Print lof shape: torch.Size([20000, 60])\n"
     ]
    }
   ],
   "source": [
    "obs_samples_lof = proposal_lof.sample((20000,))\n",
    "obs_samples_mis = proposal_mis.sample((20000,))\n",
    "\n",
    "print(\"Print missense shape: {}\".format(obs_samples_mis.shape))\n",
    "print(\"Print lof shape: {}\".format(obs_samples_lof.shape))\n",
    "\n",
    "\n",
    "\n",
    "obs_samples_lof_reshaped = obs_samples_lof.reshape(-1)\n",
    "obs_samples_mis_reshaped = obs_samples_mis.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating the mean of all samples\n",
    "mean_samps = []\n",
    "for i in range(0,1000):\n",
    "    temp = proposal.sample((5000,))\n",
    "    mean_samps.append(temp.mean(dim=0))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_samps = torch.cat(mean_samps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prior shape should be the same as the proposals: torch.Size([10, 60])\n"
     ]
    }
   ],
   "source": [
    "prior_samps =last_posterior._prior.sample((2000,)).view(-1).cpu().numpy()\n",
    "print(\"The prior shape should be the same as the proposals: {}\".format(last_posterior._prior.sample((10,)).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.xscale('log')\n",
    "sns.kdeplot(10**samps, label=\"DFE\", c='r')\n",
    "plt.title(\"Kernel Density Estimation Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "fig.legend([\"DFE\", \"Initial Proposal\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('ppc_nfe_lof_selectiion.png')\n",
    "plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.xscale('log')\n",
    "sns.histplot(10**mean_samps.cpu().numpy())\n",
    "plt.title(\"Histogram Plot of Inferred Scaled Selection -- Averaged out over multiple samples\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "plt.savefig('ppc_nfe_mis_selectiion_classifier.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.xscale('log')\n",
    "sns.histplot(10**obs_samples_mis_reshaped.cpu().numpy())\n",
    "plt.title(\"Histogram Plot Inferred of Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "plt.savefig('NFE_MIS_DFE.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe_mis= obs_samples_mis_reshaped.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe_lof= obs_samples_lof_reshaped.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-07 <= |s| < 1e-06: 0.1509767\n",
      "1e-06 <= |s| < 1e-05: 0.1437117\n",
      "1e-05 <= |s| < 0.0001: 0.1329875\n",
      "0.0001 <= |s| < 0.001: 0.1224242\n",
      "0.001 <= |s| < 0.01: 0.1104300\n",
      "0.01 <= |s| < 0.1: 0.0970933\n",
      "0.1 <= |s| < 1: 0.0852250\n",
      "1 <= |s| < 10000.0: 0.0000000\n",
      "|s| > 10000.0: 0.0000000\n"
     ]
    }
   ],
   "source": [
    "bins = [-7, -6, -5, -4, -3, -2, -1, 0, 4.0]\n",
    "for s0, s1 in zip(bins[:-1], bins[1:]):\n",
    "    the_dat=np.extract((s0 <= dfe_mis) & (dfe_mis < s1), dfe_mis)\n",
    "    prop = the_dat.shape[0]/obs_samples_mis_reshaped.shape[0]\n",
    "    print(f\"{10**s0} <= |s| < {10**s1}: {prop:.7f}\")\n",
    "    if s1 == bins[-1]:\n",
    "        the_dat=np.extract(dfe_mis > s1, dfe_mis)\n",
    "        prop = the_dat.shape[0]/500000.0\n",
    "        print(f\"|s| > {10**s1}: {prop:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-07 <= |s| < 1e-06: 0.1864025\n",
      "1e-06 <= |s| < 1e-05: 0.2308900\n",
      "1e-05 <= |s| < 0.0001: 0.2467042\n",
      "0.0001 <= |s| < 0.001: 0.2379450\n",
      "0.001 <= |s| < 0.01: 0.2056867\n",
      "0.01 <= |s| < 0.1: 0.1539450\n",
      "0.1 <= |s| < 1: 0.0852250\n",
      "1 <= |s| < 10000.0: 0.0000000\n",
      "|s| > 10000.0: 0.0000000\n"
     ]
    }
   ],
   "source": [
    "bins = [-7, -6, -5, -4, -3, -2, -1, 0, 4.0]\n",
    "for s0, s1 in zip(bins[:-1], bins[1:]):\n",
    "    the_dat=np.extract((s0 <= dfe_mis) & (dfe_lof < s1), dfe_lof)\n",
    "    prop = the_dat.shape[0]/obs_samples_lof_reshaped.shape[0]\n",
    "    print(f\"{10**s0} <= |s| < {10**s1}: {prop:.7f}\")\n",
    "    if s1 == bins[-1]:\n",
    "        the_dat=np.extract(dfe_lof > s1, dfe_lof)\n",
    "        prop = the_dat.shape[0]/500000.0\n",
    "        print(f\"|s| > {10**s1}: {prop:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss between true and predicted in poisson log-liklihood\n",
    "loss = -1*torch.nn.functional.poisson_nll_loss(torch.log(torch.tensor(mean_predicted2+1)), torch.log(torch.tensor(true_x+1)),log_input=True, full=False, reduction='sum' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predicted = torch.log(torch.tensor(new_predicted_fs).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_target = torch.log(torch.tensor(true_x+1).repeat(log_predicted.shape[0],1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = -1*torch.nn.functional.poisson_nll_loss(log_predicted, log_target, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisson log-liklihood loss -4795.529159783006 vs moments log-liklihood loss for lof -237.79096099886482\n"
     ]
    }
   ],
   "source": [
    "print(\"poisson log-liklihood loss {} vs moments log-liklihood loss for lof {}\".format(loss, moments_loss_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisson log-liklihood loss over batch -47.59079092120809 vs moments log-liklihood loss for lof -237.79096099886482\n"
     ]
    }
   ],
   "source": [
    "print(\"poisson log-liklihood loss over batch {} vs moments log-liklihood loss for lof {}\".format(loss2, moments_loss_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (mean_predicted - true_x)/np.sqrt(mean_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=resid, label=\"Residual\")\n",
    "plt.title(\"Poisson Residuals\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('resid_lof_32_coefficients_round10.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.5309, 5.0752, 4.4188, 4.0943, 3.6376, 3.1781, 3.1781, 2.6391, 2.5649,\n",
       "         2.7726, 2.7081, 2.7081, 2.7081, 2.7726, 2.3026, 1.0986, 2.3026, 2.3979,\n",
       "         1.6094, 1.9459, 1.3863, 1.6094, 1.3863, 1.9459, 1.9459, 1.7918, 1.3863,\n",
       "         1.6094, 1.0986, 1.0986, 1.6094, 1.7918, 2.0794, 1.6094, 1.0986, 1.6094,\n",
       "         2.0794, 0.6931, 1.0986, 1.3863, 1.3863, 0.6931, 1.0986, 0.6931, 0.0000,\n",
       "         1.0986, 0.6931, 1.0986, 0.6931, 0.6931, 0.0000, 1.0986, 1.0986, 0.6931,\n",
       "         1.0986, 1.0986, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931,\n",
       "         0.0000, 0.6931, 0.6931, 0.6931, 0.0000, 0.0000, 0.6931, 0.6931, 0.0000,\n",
       "         0.6931, 0.6931, 1.3863, 0.6931, 0.6931, 0.0000, 0.6931, 0.6931, 0.0000,\n",
       "         0.0000, 0.0000, 1.3863, 0.6931, 0.6931, 0.6931, 0.0000, 0.6931, 0.6931,\n",
       "         1.0986, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0986,\n",
       "         0.0000, 0.0000, 0.0000, 0.6931, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931,\n",
       "         0.0000, 0.6931, 0.6931, 0.0000, 0.0000, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "         0.0000, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 1.0986, 0.0000, 0.0000, 0.6931, 0.6931, 1.0986, 0.6931, 0.6931,\n",
       "         0.0000, 0.0000, 0.0000, 0.6931, 0.0000, 0.6931, 0.6931, 1.0986, 0.0000,\n",
       "         0.0000, 0.0000, 1.0986, 0.0000, 0.6931, 0.0000, 0.6931]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c011ed9bb21179cd579adf84b3829b40600ff69d12d61fe328e5e2fbe10069c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
