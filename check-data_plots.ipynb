{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi, SNLE, MNLE, SNRE, SNRE_A\n",
    "from sbi.utils.posterior_ensemble import NeuralPosteriorEnsemble\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils import MultipleIndependent\n",
    "from sbi.neural_nets.embedding_nets import PermutationInvariantEmbedding, FCEmbedding\n",
    "from sbi.utils.user_input_checks import process_prior, process_simulator\n",
    "from sbi.utils import get_density_thresholder, RestrictedPrior\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "import numpy as np\n",
    "import moments\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import atexit\n",
    "import torch.nn.functional as F\n",
    "import subprocess\n",
    "import sparselinear as sl\n",
    "from sortedcontainers import SortedDict\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "import re\n",
    "from monarch_linear import MonarchLinear\n",
    "import pdb\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR) # See: https://github.com/matplotlib/matplotlib/issues/14523\n",
    "from collections import defaultdict\n",
    "from sbi.analysis import pairplot\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=85\n",
    "the_device='cuda'\n",
    "moments_loss_lof = -237.79096099886482\n",
    "moments_loss_mis = -1065.0798831623356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_datafrom_hdf5(path_to_sim_file: str):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        path_to_sim_file (str): _description_\n",
    "    \"\"\"    \n",
    "    #TODO probably will be better to use https://github.com/quantopian/warp_prism for faster look-up tables\n",
    "    global loaded_file \n",
    "    global loaded_file_keys\n",
    "    global loaded_tree\n",
    "    import h5py\n",
    "    loaded_file = h5py.File(path_to_sim_file, 'r')\n",
    "    loaded_file_keys = list(loaded_file.keys())\n",
    "    loaded_tree = KDTree(np.asarray(loaded_file_keys)[:,None]) # needs to have a column dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sim_data(prior: float) -> torch.float32:\n",
    "\n",
    "    data = np.zeros((sample_size*2-1))\n",
    "    #theprior = prior[:-1] # last dim is misidentification\n",
    "    theprior=prior\n",
    "    #mis_id = prior[-1].cpu().numpy()\n",
    "    mis_id=0.0021\n",
    "    for a_prior in theprior:\n",
    "        _, idx = loaded_tree.query(a_prior.cpu().numpy(), k=(1,)) # the k sets number of neighbors, while we only want 1, we need to make sure it returns an array that can be indexed\n",
    "        fs = loaded_file[loaded_file_keys[idx[0]]][:]*1164.3148344084038 #15583.437265450002  # lof scaling parameter\n",
    "        fs = (1 - mis_id)*fs + mis_id * fs[::-1]\n",
    "        data += fs \n",
    "    data /= theprior.shape[0]\n",
    "    return torch.nn.functional.relu(torch.tensor(data, device=the_device)).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sim_data(prior: float) -> torch.float32:\n",
    "\n",
    "    data = np.zeros((sample_size*2-1))\n",
    "    theprior = prior[:-1] # last dim is misidentification\n",
    "    #theprior=prior\n",
    "    mis_id = prior[-1].cpu().numpy()\n",
    "    #mis_id=0.0021\n",
    "    for a_prior in theprior:\n",
    "        _, idx = loaded_tree.query(a_prior.cpu().numpy(), k=(1,)) # the k sets number of neighbors, while we only want 1, we need to make sure it returns an array that can be indexed\n",
    "        fs = loaded_file[loaded_file_keys[idx[0]]][:]*1164.3148344084038 #15583.437265450002  # lof scaling parameter\n",
    "        fs = (1 - mis_id)*fs + mis_id * fs[::-1]\n",
    "        data += fs \n",
    "    data /= theprior.shape[0]\n",
    "    return torch.nn.functional.relu(torch.tensor(data, device=the_device)).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moments_sim_data2(prior: float) -> torch.float32:\n",
    "    \n",
    "    global sample_size\n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    #s_prior, weights = prior[:6], prior[6:]\n",
    "    #s_prior, weights = prior[:5], prior[5:]\n",
    "    #s_prior, p_misid, weights = prior[:7], prior[7], prior[7:]\n",
    "    fs_aggregate = None\n",
    "    p_misid = 0.0021 #.0021 # lof missid # 0.0147 missense\n",
    "    gammas = -1*10**(prior.cpu().numpy().squeeze())\n",
    "    nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "                np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        while rerun:\n",
    "            #print(gamma, j)\n",
    "            ns_sim = 2 * ns_sim\n",
    "            fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "            fs = moments.Spectrum(fs)\n",
    "            fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "            \n",
    "            fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "            if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "                # large gamma-values can require large sample sizes for stability\n",
    "                rerun = True\n",
    "                del fs\n",
    "            else:\n",
    "                rerun = False\n",
    "        if j == 0:\n",
    "            fs_aggregate = fs.project([projected_sample_size]).compressed()*theta_lof\n",
    "            #check_test_2.append(fs_aggregate)\n",
    "        else:\n",
    "            fs2 = fs.project([projected_sample_size]).compressed()*theta_lof\n",
    "            fs_aggregate += fs2\n",
    "            #check_test_2.append(fs2)\n",
    "            del fs2\n",
    "        rerun = True\n",
    "        ns_sim = 100\n",
    "    \n",
    "    #check_fs_aggregate = np.copy(fs_aggregate)            \n",
    "    fs_aggregate /= gammas.shape[0]\n",
    "    #fs_aggregate = torch.poisson(torch.nn.functional.relu(torch.tensor(fs_aggregate))).type(torch.float32) \n",
    "    fs_aggregate = torch.nn.functional.relu(torch.tensor(fs_aggregate)).type(torch.float32) \n",
    "    #return gammas, check_test_2, check_fs_aggregate, fs_aggregate\n",
    "    return fs_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_momments(prior: float, sample_size) -> torch.float32:\n",
    "    \n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    gamma = -1*10**(prior)\n",
    "    p_misid = 0.0 #.0021 # lof missid\n",
    "\n",
    "    while rerun:\n",
    "        ns_sim = 2 * ns_sim\n",
    "        fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "        fs = moments.Spectrum(fs)\n",
    "        fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "        nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "            np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "        fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "        if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "            # large gamma-values can require large sample sizes for stability\n",
    "            rerun = True\n",
    "            print(\"rerunning\")\n",
    "        else:\n",
    "            rerun = False\n",
    "        fs2 = fs.project([projected_sample_size]).compressed()*theta_lof\n",
    "        fs2 = (1 - p_misid) * fs2 + p_misid * fs2[::-1]\n",
    "        \n",
    "    return fs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters from moments, gamma for lof\n",
    "# shape: 0.3589\n",
    "# scale: 7830.5\n",
    "gdist = torch.distributions.gamma.Gamma(torch.tensor([0.3589]),torch.tensor([1/7830.5]))\n",
    "gamma_samples = gdist.sample((1000,)).type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal parameters (missense):\n",
    "#shape: 0.4448\n",
    "#scale: 82.4\n",
    "gdist2 = torch.distributions.gamma.Gamma(torch.tensor([0.4448]),torch.tensor([1/82.4]))\n",
    "gamma_samples = gdist.sample((1000,)).type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udist = torch.distributions.uniform.Uniform(-6.0*torch.ones(50),4.0*torch.ones(50))\n",
    "uniform_samples = udist.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_posterior = torch.load('Experiments/saved_posteriors_msl_mis_scf_sinkhorn_36_sel_blur_2_2023-04-11_14-06/posterior_observed_round_2.pkl')\n",
    "last_posterior=torch.load('Experiments/saved_posteriors_msl_lof_scf_sinkhorn_12_pmsid_and_optimizer_2023-04-17_11-50/posterior_observed_round_2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_x = np.load('emperical_missense_sfs_msl.npy')\n",
    "true_x = np.load('emperical_lof_sfs_msl.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rerunning\n",
      "rerunning\n"
     ]
    }
   ],
   "source": [
    "check_aggregate = generate_momments(2.2, 85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Selection Coefficient: [564  27   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "True X: [685. 159.  82.  59.  37.  23.  23.  13.  12.  15.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Single Selection Coefficient: {check_aggregate[:20].astype(int)}\")\n",
    "print(\"True X: {}\".format(true_x[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_reject_fn = get_density_thresholder(last_posterior, quantile=1e-5, num_samples_to_estimate_support=100000)\n",
    "proposal = RestrictedPrior(last_posterior._prior, accept_reject_fn, last_posterior, sample_with=\"sir\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]\n",
    "predicted_fs2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples = proposal.sample((1000,), oversampling_factor=1024)[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with p_misid\n",
    "obs_samples2 = proposal.sample((1000,), oversampling_factor=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.1886, -2.9545, -0.9737, -5.9108,  3.8404, -5.8858, -1.5907, -3.3094,\n",
       "        -2.3612, -5.9933, -2.2722], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(obs_samples,dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sampled selection coefficients: torch.Size([1000, 11])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of sampled selection coefficients: {}\".format(obs_samples.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF using non-cached data\n",
    "for obs_sample in tqdm(obs_samples):\n",
    "    fs = generate_moments_sim_data2(obs_sample)\n",
    "    predicted_fs.append(fs.unsqueeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sim_datafrom_hdf5('moments_msl_sfs_lof_hdf5_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 208.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for obs_sample in tqdm(obs_samples):\n",
    "    fs = generate_sim_data(obs_sample)\n",
    "    predicted_fs.append(fs.unsqueeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:08<00:00, 113.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for obs_sample in tqdm(obs_samples2):\n",
    "    fs = generate_sim_data2(obs_sample)\n",
    "    predicted_fs2.append(fs.unsqueeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency spectrum containing all bins (1000, 169)\n"
     ]
    }
   ],
   "source": [
    "new_predicted_fs = np.asarray(predicted_fs).squeeze(1)\n",
    "print(\"Shape of frequency spectrum containing all bins {}\".format(new_predicted_fs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency spectrum containing all bins (1000, 169)\n"
     ]
    }
   ],
   "source": [
    "new_predicted_fs2 = np.asarray(predicted_fs2).squeeze(1)\n",
    "print(\"Shape of frequency spectrum containing all bins {}\".format(new_predicted_fs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_true_x = np.log10(true_x[1:169:10])\n",
    "idx = np.arange(1,169,10)\n",
    "print(\"Shape of indices (bin) that were chosen to plot {}\".format(idx.shape[0]))\n",
    "print(\"Showing idx for sanity check\\n\")\n",
    "print(idx)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 25):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    sns.histplot(np.log10(new_predicted_fs[:,i-1]))\n",
    "    plt.axvline(x=np.mean(np.log10(new_predicted_fs[:,i-1])), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(np.log10(new_predicted_fs[:,i-1])), color='k', label=\"median\")\n",
    "    ax.axline((np.log10(true_x[i-1]), 1), (np.log10(true_x[i-1]),100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(np.log10(true_x[i-1]),i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_mis_36_coefficients_round2_blur_2_hs.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 25):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    sns.histplot(np.log10(new_predicted_fs2[:,i-1]))\n",
    "    plt.axvline(x=np.mean(np.log10(new_predicted_fs2[:,i-1])), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(np.log10(new_predicted_fs2[:,i-1])), color='k', label=\"median\")\n",
    "    ax.axline((np.log10(true_x[i-1]), 1), (np.log10(true_x[i-1]),100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(np.log10(true_x[i-1]),i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_lof_36_coefficients_round2_blur_2_s.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without pmsid: [2935.0466  1176.6456   713.8925   509.01065  393.62485  319.29532\n",
      "  267.25986  228.74918  199.09727  175.5803 ]\n",
      "[685. 159.  82.  59.  37.  23.  23.  13.  12.  15.]\n"
     ]
    }
   ],
   "source": [
    "mean_predicted = np.mean(new_predicted_fs[:],axis=0)\n",
    "print(f\"Without pmsid: {mean_predicted[:10]}\")\n",
    "print(true_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With pmsid: [1872.4374   743.6381   450.16882  320.6129   247.7485   200.84918\n",
      "  168.03575  143.76195  125.07932  110.26723]\n",
      "[685. 159.  82.  59.  37.  23.  23.  13.  12.  15.]\n"
     ]
    }
   ],
   "source": [
    "mean_predicted2 = np.mean(new_predicted_fs2[:],axis=0)\n",
    "print(f\"With pmsid: {mean_predicted2[:10]}\")\n",
    "print(true_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to create a normalized batch tensor of the predicted frequency spectrum\n",
    "temp = torch.tensor(predicted_fs)\n",
    "temp = temp.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_predicted_fs = temp/temp.sum(dim=1).view(temp.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp[0,:10])\n",
    "print(temp.sum(dim=1)[0])\n",
    "print(norm_predicted_fs[0,:10])\n",
    "# Test to create a normalized batch tensor of the predicted frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,mean_predicted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=np.log10(mean_predicted+1), label=\"Predicted\")\n",
    "sns.scatterplot(x=x,y=np.log10(true_x+1), label=\"Emperical\")\n",
    "plt.title(\"Mean of posterior predicted SFS vs True SFS\")\n",
    "plt.ylabel(\"Log Scaled Allele Frequency\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('ppc_scatter_lof_12_coefficients_round5.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=np.log10(mean_predicted2+1), label=\"Predicted\")\n",
    "sns.scatterplot(x=x,y=np.log10(true_x+1), label=\"Emperical\")\n",
    "plt.title(\"Mean of posterior predicted SFS vs True SFS\")\n",
    "plt.ylabel(\"Log Scaled Allele Frequency\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('ppc_scatter_mis_36_coefficients_round1.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 11])\n",
      "tensor([-5.1886, -2.9545, -0.9737, -5.9108,  3.8404, -5.8858, -1.5907, -3.3094,\n",
      "        -2.3612, -5.9933, -2.2722], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(obs_samples.shape)\n",
    "print(obs_samples.mean(dim=0))\n",
    "obs_samples2 = obs_samples.reshape(-1)\n",
    "\n",
    "samps = obs_samples2.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samps =last_posterior._prior.sample((2000,)).view(-1).cpu().numpy()\n",
    "gamma_samples2 = gdist.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.kdeplot(samps, label=\"DFE\", c='r')\n",
    "sns.kdeplot(prior_samps, label=\"Initial Proposal\", c='g')\n",
    "sns.kdeplot(torch.log10(gamma_samples2), label=\"Moments Proposal\")\n",
    "plt.title(\"Kernel Density Estimation Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "fig.legend([\"DFE\", \"Initial Proposal\", \"Moments Proposal\"], loc=\"lower center\", ncol=3)\n",
    "plt.savefig('ppc_mmd_kde_selection_lof_12_coefficients_round5.png')\n",
    "plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.kdeplot(samps, label=\"DFE\", c='r')\n",
    "sns.kdeplot(prior_samps, label=\"Initial Proposal\", c='g')\n",
    "sns.kdeplot(torch.log10(gamma_samples2), label=\"Moments Proposal\")\n",
    "plt.title(\"Kernel Density Estimation Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "fig.legend([\"DFE\", \"Initial Proposal\", \"Moments Proposal\"], loc=\"lower center\", ncol=3)\n",
    "plt.savefig('ppc_mmd_kde_selection_mis_36_coefficients_round2_blur_1_s.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe2= samps.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06 <= |s| < 1e-05: 0.0000000\n",
      "1e-05 <= |s| < 0.0001: 0.0000000\n",
      "0.0001 <= |s| < 0.001: 0.0000000\n",
      "0.001 <= |s| < 0.01: 0.3749375\n",
      "0.01 <= |s| < 0.1: 0.0915938\n",
      "0.1 <= |s| < 1: 0.0851250\n",
      "1 <= |s| < 10000.0: 0.4483437\n",
      "|s| > 10000.0: 0.0000000\n"
     ]
    }
   ],
   "source": [
    "bins = [-6, -5, -4, -3, -2, -1, 0, 4.0]\n",
    "for s0, s1 in zip(bins[:-1], bins[1:]):\n",
    "    the_dat=np.extract((s0 <= dfe2) & (dfe2 < s1), dfe2)\n",
    "    prop = the_dat.shape[0]/obs_samples2.shape[0]\n",
    "    print(f\"{10**s0} <= |s| < {10**s1}: {prop:.7f}\")\n",
    "    if s1 == bins[-1]:\n",
    "        the_dat=np.extract(dfe2 > s1, dfe2)\n",
    "        prop = the_dat.shape[0]/500000.0\n",
    "        print(f\"|s| > {10**s1}: {prop:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss between true and predicted in poisson log-liklihood\n",
    "loss = -1*torch.nn.functional.poisson_nll_loss(torch.log(torch.tensor(mean_predicted2+1)), torch.log(torch.tensor(true_x+1)),log_input=True, full=False, reduction='sum' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predicted = torch.log(torch.tensor(new_predicted_fs).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_target = torch.log(torch.tensor(true_x+1).repeat(log_predicted.shape[0],1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = -1*torch.nn.functional.poisson_nll_loss(log_predicted, log_target, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisson log-liklihood loss -4795.529159783006 vs moments log-liklihood loss for lof -237.79096099886482\n"
     ]
    }
   ],
   "source": [
    "print(\"poisson log-liklihood loss {} vs moments log-liklihood loss for lof {}\".format(loss, moments_loss_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisson log-liklihood loss over batch -47.59079092120809 vs moments log-liklihood loss for lof -237.79096099886482\n"
     ]
    }
   ],
   "source": [
    "print(\"poisson log-liklihood loss over batch {} vs moments log-liklihood loss for lof {}\".format(loss2, moments_loss_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (mean_predicted - true_x)/np.sqrt(mean_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=resid, label=\"Residual\")\n",
    "plt.title(\"Poisson Residuals\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('resid_lof_32_coefficients_round10.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.5309, 5.0752, 4.4188, 4.0943, 3.6376, 3.1781, 3.1781, 2.6391, 2.5649,\n",
       "         2.7726, 2.7081, 2.7081, 2.7081, 2.7726, 2.3026, 1.0986, 2.3026, 2.3979,\n",
       "         1.6094, 1.9459, 1.3863, 1.6094, 1.3863, 1.9459, 1.9459, 1.7918, 1.3863,\n",
       "         1.6094, 1.0986, 1.0986, 1.6094, 1.7918, 2.0794, 1.6094, 1.0986, 1.6094,\n",
       "         2.0794, 0.6931, 1.0986, 1.3863, 1.3863, 0.6931, 1.0986, 0.6931, 0.0000,\n",
       "         1.0986, 0.6931, 1.0986, 0.6931, 0.6931, 0.0000, 1.0986, 1.0986, 0.6931,\n",
       "         1.0986, 1.0986, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931,\n",
       "         0.0000, 0.6931, 0.6931, 0.6931, 0.0000, 0.0000, 0.6931, 0.6931, 0.0000,\n",
       "         0.6931, 0.6931, 1.3863, 0.6931, 0.6931, 0.0000, 0.6931, 0.6931, 0.0000,\n",
       "         0.0000, 0.0000, 1.3863, 0.6931, 0.6931, 0.6931, 0.0000, 0.6931, 0.6931,\n",
       "         1.0986, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0986,\n",
       "         0.0000, 0.0000, 0.0000, 0.6931, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931,\n",
       "         0.0000, 0.6931, 0.6931, 0.0000, 0.0000, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "         0.0000, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 1.0986, 0.0000, 0.0000, 0.6931, 0.6931, 1.0986, 0.6931, 0.6931,\n",
       "         0.0000, 0.0000, 0.0000, 0.6931, 0.0000, 0.6931, 0.6931, 1.0986, 0.0000,\n",
       "         0.0000, 0.0000, 1.0986, 0.0000, 0.6931, 0.0000, 0.6931]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c011ed9bb21179cd579adf84b3829b40600ff69d12d61fe328e5e2fbe10069c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
