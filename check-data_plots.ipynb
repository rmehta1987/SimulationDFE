{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi, SNLE, MNLE, SNRE, SNRE_A\n",
    "from sbi.utils.posterior_ensemble import NeuralPosteriorEnsemble\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils import MultipleIndependent\n",
    "from sbi.neural_nets.embedding_nets import PermutationInvariantEmbedding, FCEmbedding\n",
    "from sbi.utils.user_input_checks import process_prior, process_simulator\n",
    "from sbi.utils import get_density_thresholder, RestrictedPrior\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "import numpy as np\n",
    "import moments\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import atexit\n",
    "import torch.nn.functional as F\n",
    "import subprocess\n",
    "import sparselinear as sl\n",
    "from sortedcontainers import SortedDict\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "import re\n",
    "from monarch_linear import MonarchLinear\n",
    "import pdb\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR) # See: https://github.com/matplotlib/matplotlib/issues/14523\n",
    "from collections import defaultdict\n",
    "from sbi.analysis import pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportanceSamplingEstimator(sample, threshold, target=None, num_particles=None):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sample (_type_): _description_\n",
    "        target (_type_): _description_\n",
    "        threshold (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    cdftest = target.q.transforms\n",
    "    low_samples = sample - .0025\n",
    "    high_samples = sample + .0025 \n",
    "    num_particles = 1000 if num_particles is None else num_particles\n",
    "    if target is not None:\n",
    "        with torch.no_grad():\n",
    "            for transform in cdftest[::-1]:\n",
    "                value = transform.inv(high_samples)\n",
    "            if target.q._validate_args:\n",
    "                target.q.base_dist._validate_sample(value)\n",
    "            value = target.q.base_dist.base_dist.cdf(value)\n",
    "            #value = target.q._monotonize_cdf(value)\n",
    "        with torch.no_grad():\n",
    "            for transform in cdftest[::-1]:\n",
    "                value2 = transform.inv(low_samples)\n",
    "            if target.q._validate_args:\n",
    "                target.q.base_dist._validate_sample(value2)\n",
    "            value2 = target.q.base_dist.base_dist.cdf(value2)\n",
    "            #value2 = target.q.base_dist._monotonize_cdf(value2)\n",
    "        \n",
    "        return value - value2\n",
    "    else:\n",
    "        sample_low = sample-threshold\n",
    "        sample_high = sample+threshold\n",
    "        proposal = torch.distributions.uniform.Uniform(sample_low, sample_high)\n",
    "        prop_samps = proposal.sample((num_particles,))\n",
    "        target_logprobs = target.log_prob(prop_samps)\n",
    "        proposal_logprobs = proposal.log_prob(prop_samps)\n",
    "        log_importance_weights = target_logprobs - proposal_logprobs\n",
    "\n",
    "    ret = torch.sum(torch.exp(log_importance_weights))/num_particles\n",
    "\n",
    "\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moments_sim_data(prior: float) -> torch.float32:\n",
    "    \n",
    "    global sample_size\n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    #s_prior, weights = prior[:6], prior[6:]\n",
    "    #s_prior, weights = prior[:5], prior[5:]\n",
    "    s_prior, p_misid, weights = prior[:7], prior[7], prior[7:]\n",
    "    fs_aggregate = None\n",
    "    gammas = s_prior.cpu().numpy().squeeze()\n",
    "    weights = weights.cpu().numpy().squeeze()\n",
    "    p_misid = p_misid.cpu().numpy()\n",
    "    for j, (gamma, weight) in enumerate(zip(gammas, weights)):\n",
    "        while rerun:\n",
    "            ns_sim = 2 * ns_sim\n",
    "            fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "            fs = moments.Spectrum(fs)\n",
    "            fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "            nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "                np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "            fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "            if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "                # large gamma-values can require large sample sizes for stability\n",
    "                rerun = True\n",
    "            else:\n",
    "                rerun = False\n",
    "        if j == 0:\n",
    "            fs_aggregate = fs.project([projected_sample_size]).compressed()*theta_mis * weight\n",
    "            fs_aggregate = (1 - p_misid)*fs_aggregate + p_misid * fs_aggregate[::-1]\n",
    "        else:\n",
    "            fs_aggregate += fs.project([projected_sample_size]).compressed()*theta_mis * weight\n",
    "            fs_aggregate = (1 - p_misid)*fs_aggregate + p_misid * fs_aggregate[::-1]\n",
    "\n",
    "    fs_aggregate = torch.tensor(fs_aggregate).type(torch.float32) \n",
    "    return fs_aggregate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moments_sim_data2(prior: float) -> torch.float32:\n",
    "    \n",
    "    global sample_size\n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    fs_aggregate = None\n",
    "    gammas = prior.cpu().numpy().squeeze()\n",
    "   \n",
    "    for j, gamma in enumerate(gammas):\n",
    "        while rerun:\n",
    "            ns_sim = 2 * ns_sim\n",
    "            fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "            fs = moments.Spectrum(fs)\n",
    "            fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "            nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "                np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "            fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "            if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "                # large gamma-values can require large sample sizes for stability\n",
    "                rerun = True\n",
    "            else:\n",
    "                rerun = False\n",
    "        if j == 0:\n",
    "            fs_aggregate = fs.project([projected_sample_size]).compressed()*theta_mis\n",
    "        else:\n",
    "            fs2 = fs.project([projected_sample_size]).compressed()*theta_mis\n",
    "            fs_aggregate += fs2\n",
    "            del fs2\n",
    "            \n",
    "    fs_aggregate /= gammas.shape[0]\n",
    "    fs_aggregate = torch.poisson(torch.tensor(fs_aggregate)).type(torch.float32) \n",
    "    return fs_aggregate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_posterior = torch.load('Experiments/saved_posteriors_msl_mcf_6_and_psmid_params_2023-04-05_20-19/posterior_observed_last_round.pkl')\n",
    "true_x = np.load('emperical_missense_sfs_msl.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_reject_fn = get_density_thresholder(last_posterior, quantile=1e-5, num_samples_to_estimate_support=10000)\n",
    "proposal = RestrictedPrior(last_posterior._prior, accept_reject_fn, last_posterior, sample_with=\"sir\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]\n",
    "predicted_fs2=[]\n",
    "sample_size = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples = proposal.sample((2000,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3912,  6.7486, -5.8919, -7.5726,  0.8809, -7.7729,  2.0690,  1.5029,\n",
       "         6.4421,  5.5723,  5.2281,  0.1243,  6.3237,  6.1314, -8.6591,  5.1889,\n",
       "        -3.6221,  5.0881,  2.9969, -0.8614,  3.8105,  0.7566,  1.2305, -8.4798,\n",
       "        -4.2378, -3.2068, -0.6270, -4.6329, -7.0010, -5.5889,  5.3062,  5.7948,\n",
       "         5.4552, -5.6743,  4.7063, -4.5431, -0.7692,  4.1205,  2.2638,  5.6893,\n",
       "         0.6888, -0.5392, -8.3251,  5.2313,  1.1143, -5.4134,  5.9483, -0.6425,\n",
       "         2.4292, -3.7556], device='cuda:0')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_samples[:50,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sampled selection coefficients: torch.Size([2000, 20])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of sampled selection coefficients: {}\".format(obs_samples.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs_sample in obs_samples:\n",
    "    \n",
    "    fs = generate_moments_sim_data2(obs_sample)\n",
    "    predicted_fs.append(fs.unsqueeze(0).cpu().numpy())\n",
    "    predicted_fs2.append(np.log10(fs[1:169:10].unsqueeze(0).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency spectrum containing all bins (2000, 169)\n"
     ]
    }
   ],
   "source": [
    "new_predicted_fs = np.asarray(predicted_fs[2000:]).squeeze(1)\n",
    "print(\"Shape of frequency spectrum containing all bins {}\".format(new_predicted_fs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_true_x = np.log10(true_x[:17])\n",
    "#new_predicted_fs2 = np.asarray(predicted_fs2[2000:]).squeeze(1)\n",
    "new_predicted_fs2 = np.log10(new_predicted_fs[:,:17])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of indices (bin) that were chosen to plot 17\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(1,169,10)\n",
    "print(\"Shape of indices (bin) that were chosen to plot {}\".format(idx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  11,  21,  31,  41,  51,  61,  71,  81,  91, 101, 111, 121,\n",
       "       131, 141, 151, 161])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many subplots to create per bin: (17,)\n",
      "Shape of predicted SFS: (2000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"How many subplots to create per bin: {}\".format(smaller_true_x.shape))\n",
    "print(\"Shape of predicted SFS: {}\".format(new_predicted_fs2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "x_points = np.arange(1, smaller_true_x.shape[0])\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 8):\n",
    "    ax = fig.add_subplot(3, 3, i)\n",
    "    #sns.scatterplot(ax=ax, x=predicted_fs2[:,i-1], y=x_points[i-1])\n",
    "    sns.histplot(new_predicted_fs2[:,i-1])\n",
    "    plt.axvline(x=np.mean(new_predicted_fs2[:,i-1]), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(new_predicted_fs2[:,i-1]), color='k', label=\"median\")\n",
    "    ax.axline((smaller_true_x[i-1], 1), (smaller_true_x[i-1],100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    true_diff_predicted = 10**(np.mean(new_predicted_fs2[:,i-1])) - 10**(smaller_true_x[i-1])\n",
    "    plt.text(x=((np.mean(new_predicted_fs2[:,i-1])+smaller_true_x[i-1])/2), y=50, s=\"{:.2f}.\".format(true_diff_predicted))\n",
    "    #ax.plot(smaller_true_x[i-1], x_points[i-1], markersize=7.0, c='r', marker='+')\n",
    "    #plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(smaller_true_x[i-1], idx[i-1] ))\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(smaller_true_x[i-1],i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_averaged_samples_is_20.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26296.,  8870.,  4960.,  3369.,  2599.,  2021.,  1563.,  1347.,\n",
       "        1168.,  1057.])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38460., 12776.,  6312.,  3560.,  2270.,  1564.,  1015.,   728.,\n",
       "         538.,   348.], dtype=float32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predicted_fs[0,:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c011ed9bb21179cd579adf84b3829b40600ff69d12d61fe328e5e2fbe10069c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
