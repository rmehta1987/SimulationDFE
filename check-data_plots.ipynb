{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi, SNLE, MNLE, SNRE, SNRE_A\n",
    "from sbi.utils.posterior_ensemble import NeuralPosteriorEnsemble\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils import MultipleIndependent\n",
    "from sbi.neural_nets.embedding_nets import PermutationInvariantEmbedding, FCEmbedding\n",
    "from sbi.utils.user_input_checks import process_prior, process_simulator\n",
    "from sbi.utils import get_density_thresholder, RestrictedPrior\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "import numpy as np\n",
    "import moments\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import atexit\n",
    "import torch.nn.functional as F\n",
    "import subprocess\n",
    "import sparselinear as sl\n",
    "from sortedcontainers import SortedDict\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "import re\n",
    "from monarch_linear import MonarchLinear\n",
    "import pdb\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR) # See: https://github.com/matplotlib/matplotlib/issues/14523\n",
    "from collections import defaultdict\n",
    "from sbi.analysis import pairplot\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=85\n",
    "the_device='cuda'\n",
    "moments_loss_lof = -237.79096099886482\n",
    "moments_loss_mis = -1065.0798831623356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_datafrom_hdf5(path_to_sim_file: str):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        path_to_sim_file (str): _description_\n",
    "    \"\"\"    \n",
    "    #TODO probably will be better to use https://github.com/quantopian/warp_prism for faster look-up tables\n",
    "    global loaded_file \n",
    "    global loaded_file_keys\n",
    "    global loaded_tree\n",
    "    import h5py\n",
    "    loaded_file = h5py.File(path_to_sim_file, 'r')\n",
    "    loaded_file_keys = list(loaded_file.keys())\n",
    "    loaded_tree = KDTree(np.asarray(loaded_file_keys)[:,None]) # needs to have a column dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sim_data(prior: float) -> torch.float32:\n",
    "\n",
    "    data = np.zeros((sample_size*2-1))\n",
    "    #theprior = prior[:-1] # last dim is misidentification\n",
    "    theprior=prior\n",
    "    #mis_id = prior[-1].cpu().numpy()\n",
    "    #mis_id=0.0021\n",
    "    for a_prior in theprior:\n",
    "        _, idx = loaded_tree.query(a_prior.cpu().numpy(), k=(1,)) # the k sets number of neighbors, while we only want 1, we need to make sure it returns an array that can be indexed\n",
    "        #fs = loaded_file[loaded_file_keys[idx[0]]][:]*1164.3148344084038 #15583.437265450002  # lof scaling parameter\n",
    "        fs = loaded_file[loaded_file_keys[idx[0]]][:]*15583.437265450002  # mis scaling parameter\n",
    "        fs = (1 - mis_id)*fs + mis_id * fs[::-1]\n",
    "        data += fs \n",
    "    data /= theprior.shape[0]\n",
    "    return torch.nn.functional.relu(torch.tensor(data, device=the_device)).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sim_data(prior: float) -> torch.float32:\n",
    "\n",
    "    data = np.zeros((sample_size*2-1))\n",
    "    theprior = prior[:-1] # last dim is misidentification\n",
    "    #theprior=prior\n",
    "    mis_id = prior[-1].cpu().numpy()\n",
    "    #mis_id=0.0021\n",
    "    for a_prior in theprior:\n",
    "        _, idx = loaded_tree.query(a_prior.cpu().numpy(), k=(1,)) # the k sets number of neighbors, while we only want 1, we need to make sure it returns an array that can be indexed\n",
    "        fs = loaded_file[loaded_file_keys[idx[0]]][:]*1164.3148344084038 #15583.437265450002  # lof scaling parameter\n",
    "        fs = (1 - mis_id)*fs + mis_id * fs[::-1]\n",
    "        data += fs \n",
    "    data /= theprior.shape[0]\n",
    "    return torch.nn.functional.relu(torch.tensor(data, device=the_device)).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moments_sim_data2(prior: float) -> torch.float32:\n",
    "    \n",
    "    global sample_size\n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    #s_prior, weights = prior[:6], prior[6:]\n",
    "    #s_prior, weights = prior[:5], prior[5:]\n",
    "    #s_prior, p_misid, weights = prior[:7], prior[7], prior[7:]\n",
    "    fs_aggregate = None\n",
    "    p_misid = 0.0021 #.0021 # lof missid # 0.0147 missense\n",
    "    gammas = -1*10**(prior.cpu().numpy().squeeze())\n",
    "    nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "                np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        while rerun:\n",
    "            #print(gamma, j)\n",
    "            ns_sim = 2 * ns_sim\n",
    "            fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "            fs = moments.Spectrum(fs)\n",
    "            fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "            \n",
    "            fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "            if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "                # large gamma-values can require large sample sizes for stability\n",
    "                rerun = True\n",
    "                del fs\n",
    "            else:\n",
    "                rerun = False\n",
    "        if j == 0:\n",
    "            fs_aggregate = fs.project([projected_sample_size]).compressed()*theta_lof\n",
    "            #check_test_2.append(fs_aggregate)\n",
    "        else:\n",
    "            fs2 = fs.project([projected_sample_size]).compressed()*theta_lof\n",
    "            fs_aggregate += fs2\n",
    "            #check_test_2.append(fs2)\n",
    "            del fs2\n",
    "        rerun = True\n",
    "        ns_sim = 100\n",
    "    \n",
    "    #check_fs_aggregate = np.copy(fs_aggregate)            \n",
    "    fs_aggregate /= gammas.shape[0]\n",
    "    #fs_aggregate = torch.poisson(torch.nn.functional.relu(torch.tensor(fs_aggregate))).type(torch.float32) \n",
    "    fs_aggregate = torch.nn.functional.relu(torch.tensor(fs_aggregate)).type(torch.float32) \n",
    "    #return gammas, check_test_2, check_fs_aggregate, fs_aggregate\n",
    "    return fs_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_momments(prior: float, sample_size) -> torch.float32:\n",
    "    \n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    gamma = -1*10**(prior)\n",
    "    p_misid = 0.0 #.0021 # lof missid\n",
    "\n",
    "    while rerun:\n",
    "        ns_sim = 2 * ns_sim\n",
    "        fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "        fs = moments.Spectrum(fs)\n",
    "        fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "        nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "            np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "        fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "        if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "            # large gamma-values can require large sample sizes for stability\n",
    "            rerun = True\n",
    "            print(\"rerunning\")\n",
    "        else:\n",
    "            rerun = False\n",
    "        fs2 = fs.project([projected_sample_size]).compressed()*theta_lof\n",
    "        fs2 = (1 - p_misid) * fs2 + p_misid * fs2[::-1]\n",
    "        \n",
    "    return fs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters from moments, gamma for lof\n",
    "# shape: 0.3589\n",
    "# scale: 7830.5\n",
    "gdist = torch.distributions.gamma.Gamma(torch.tensor([0.3589]),torch.tensor([1/7830.5]))\n",
    "gamma_samples = gdist.sample((1000,)).type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal parameters (missense):\n",
    "#shape: 0.4448\n",
    "#scale: 82.4\n",
    "gdist2 = torch.distributions.gamma.Gamma(torch.tensor([0.4448]),torch.tensor([1/82.4]))\n",
    "gamma_samples = gdist2.sample((1000,)).type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal parameters (missense):\n",
    "#shape: 0.1830\n",
    "#scale: 733.6\n",
    "gdist3 = torch.distributions.gamma.Gamma(torch.tensor([0.1830]),torch.tensor([1/733.6]))\n",
    "gamma_samples = gdist3.sample((1000,)).type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udist = torch.distributions.uniform.Uniform(-6.0*torch.ones(50),4.0*torch.ones(50))\n",
    "uniform_samples = udist.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_posterior = torch.load('Experiments/saved_posteriors_msl_mis_scf_sinkhorn_36_sel_blur_2_2023-04-11_14-06/posterior_observed_round_2.pkl')\n",
    "#last_posterior=torch.load('Experiments/saved_posteriors_msl_lof_scf_sinkhorn_12_pmsid_and_optimizer_2023-04-17_11-50/posterior_observed_round_2.pkl')\n",
    "last_posterior=torch.load('msl_restriction_classifier_missense_6_coef.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_x = np.load('emperical_missense_sfs_msl.npy')\n",
    "#true_x = np.load('emperical_lof_sfs_msl.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rerunning\n",
      "rerunning\n"
     ]
    }
   ],
   "source": [
    "check_aggregate = generate_momments(2.2, 85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Selection Coefficient: [564  27   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "True X: [685. 159.  82.  59.  37.  23.  23.  13.  12.  15.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Single Selection Coefficient: {check_aggregate[:20].astype(int)}\")\n",
    "print(\"True X: {}\".format(true_x[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_reject_fn = get_density_thresholder(last_posterior, quantile=1e-5, num_samples_to_estimate_support=100000)\n",
    "proposal = RestrictedPrior(last_posterior._prior, accept_reject_fn, last_posterior, sample_with=\"sir\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = last_posterior.restrict_prior(allowed_false_negatives=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]\n",
    "predicted_fs2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples = proposal.sample((1000,), oversampling_factor=1024)[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with p_misid\n",
    "obs_samples2 = proposal.sample((1000,), oversampling_factor=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.1886, -2.9545, -0.9737, -5.9108,  3.8404, -5.8858, -1.5907, -3.3094,\n",
       "        -2.3612, -5.9933, -2.2722], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(obs_samples,dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sampled selection coefficients: torch.Size([1000, 11])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of sampled selection coefficients: {}\".format(obs_samples.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_moments_sim_data2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# IF using non-cached data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obs_sample \u001b[38;5;129;01min\u001b[39;00m tqdm(obs_samples):\n\u001b[0;32m----> 3\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_moments_sim_data2\u001b[49m(obs_sample)\n\u001b[1;32m      4\u001b[0m     predicted_fs\u001b[38;5;241m.\u001b[39mappend(fs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_moments_sim_data2' is not defined"
     ]
    }
   ],
   "source": [
    "# IF using non-cached data\n",
    "for obs_sample in tqdm(obs_samples):\n",
    "    fs = generate_moments_sim_data(obs_sample)\n",
    "    predicted_fs.append(fs.unsqueeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sim_datafrom_hdf5('moments_msl_sfs_lof_hdf5_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 331.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for obs_sample in tqdm(obs_samples):\n",
    "    fs = generate_sim_data(obs_sample)\n",
    "    predicted_fs.append(fs.unsqueeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:08<00:00, 113.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for obs_sample in tqdm(obs_samples2):\n",
    "    fs = generate_sim_data2(obs_sample)\n",
    "    predicted_fs2.append(fs.unsqueeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency spectrum containing all bins (1000, 169)\n"
     ]
    }
   ],
   "source": [
    "new_predicted_fs = np.asarray(predicted_fs).squeeze(1)\n",
    "print(\"Shape of frequency spectrum containing all bins {}\".format(new_predicted_fs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency spectrum containing all bins (1000, 169)\n"
     ]
    }
   ],
   "source": [
    "new_predicted_fs2 = np.asarray(predicted_fs2).squeeze(1)\n",
    "print(\"Shape of frequency spectrum containing all bins {}\".format(new_predicted_fs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_true_x = np.log10(true_x[1:169:10])\n",
    "idx = np.arange(1,169,10)\n",
    "print(\"Shape of indices (bin) that were chosen to plot {}\".format(idx.shape[0]))\n",
    "print(\"Showing idx for sanity check\\n\")\n",
    "print(idx)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 25):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    sns.histplot(np.log10(new_predicted_fs[:,i-1]))\n",
    "    plt.axvline(x=np.mean(np.log10(new_predicted_fs[:,i-1])), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(np.log10(new_predicted_fs[:,i-1])), color='k', label=\"median\")\n",
    "    ax.axline((np.log10(true_x[i-1]), 1), (np.log10(true_x[i-1]),100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(np.log10(true_x[i-1]),i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_mis_36_coefficients_round2_blur_2_hs.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 25):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    sns.histplot(np.log10(new_predicted_fs2[:,i-1]))\n",
    "    plt.axvline(x=np.mean(np.log10(new_predicted_fs2[:,i-1])), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(np.log10(new_predicted_fs2[:,i-1])), color='k', label=\"median\")\n",
    "    ax.axline((np.log10(true_x[i-1]), 1), (np.log10(true_x[i-1]),100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(np.log10(true_x[i-1]),i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_lof_36_coefficients_round2_blur_2_s.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without pmsid: [26360.676   9126.309   5148.7725  3507.2886  2624.9995  2076.03\n",
      "  1702.1844  1431.7715  1227.6006  1068.4204]\n",
      "[26296.  8870.  4960.  3369.  2599.  2021.  1563.  1347.  1168.  1057.]\n"
     ]
    }
   ],
   "source": [
    "mean_predicted = np.mean(new_predicted_fs[:],axis=0)\n",
    "print(f\"Without pmsid: {mean_predicted[:10]}\")\n",
    "print(true_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With pmsid: [1872.4374   743.6381   450.16882  320.6129   247.7485   200.84918\n",
      "  168.03575  143.76195  125.07932  110.26723]\n",
      "[685. 159.  82.  59.  37.  23.  23.  13.  12.  15.]\n"
     ]
    }
   ],
   "source": [
    "mean_predicted2 = np.mean(new_predicted_fs2[:],axis=0)\n",
    "print(f\"With pmsid: {mean_predicted2[:10]}\")\n",
    "print(true_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to create a normalized batch tensor of the predicted frequency spectrum\n",
    "temp = torch.tensor(predicted_fs)\n",
    "temp = temp.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_predicted_fs = temp/temp.sum(dim=1).view(temp.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp[0,:10])\n",
    "print(temp.sum(dim=1)[0])\n",
    "print(norm_predicted_fs[0,:10])\n",
    "# Test to create a normalized batch tensor of the predicted frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,mean_predicted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=np.log10(mean_predicted+1), label=\"Predicted\")\n",
    "sns.scatterplot(x=x,y=np.log10(true_x+1), label=\"Emperical\")\n",
    "plt.title(\"Mean of posterior predicted SFS vs True SFS\")\n",
    "plt.ylabel(\"Log Scaled Allele Frequency\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('ppc_scatter_lof_12_coefficients_round5.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=np.log10(mean_predicted+1), label=\"Predicted\")\n",
    "sns.scatterplot(x=x,y=np.log10(true_x+1), label=\"Emperical\")\n",
    "plt.title(\"Mean of posterior predicted SFS vs True SFS\")\n",
    "plt.ylabel(\"Log Scaled Allele Frequency\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('ppc_scatter_misseens_msl_classifier.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 6])\n",
      "tensor([0.7019, 0.7265, 0.3052, 0.6717, 0.2976, 0.4042], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(obs_samples.shape)\n",
    "print(obs_samples.mean(dim=0))\n",
    "obs_samples2 = obs_samples.reshape(-1)\n",
    "\n",
    "samps = obs_samples2.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samps =last_posterior._prior.sample((2000,)).view(-1).cpu().numpy()\n",
    "gamma_samples2 = gdist2.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_samples3 = gdist3.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6724e+02],\n",
       "        [4.4606e-01],\n",
       "        [9.0538e-04],\n",
       "        [2.2181e-01],\n",
       "        [2.9859e+01],\n",
       "        [4.8380e+02],\n",
       "        [5.3626e+00],\n",
       "        [2.1885e+02],\n",
       "        [3.9150e+01],\n",
       "        [8.4255e+02]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_samples3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_samps = []\n",
    "for i in range(0,1000):\n",
    "    temp = proposal.sample((5000,))\n",
    "    mean_samps.append(temp.mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_samps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_samps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "mean_samps = torch.cat(mean_samps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5618, 0.6870, 0.4131, 0.6768, 0.3406, 0.4509, 0.5626, 0.5354, 0.6109,\n",
       "        0.4710], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_samps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4735, -0.5929,  3.2106, -1.1347, -1.9285,  2.9368,  3.2626],\n",
       "        [-1.2396,  1.7918,  1.1637,  3.4872, -2.2809,  3.2927,  1.0203],\n",
       "        [-3.5343, -2.7675,  0.6168,  1.5958,  1.7019,  2.4001,  3.2581],\n",
       "        [ 3.9805, -0.0750,  3.0609, -0.3474,  0.7042, -3.7126,  2.2148],\n",
       "        [-4.7892,  2.1293, -2.9407,  3.8999,  1.3569,  0.6359,  3.4315],\n",
       "        [ 0.4981, -4.3214,  2.4759,  1.9498, -1.6436,  1.7238,  2.0312],\n",
       "        [ 3.7150,  1.9054, -5.3795,  1.7610,  3.3132, -3.7526, -0.3503],\n",
       "        [ 0.0686,  1.1194,  2.5973, -4.9369,  0.9298,  0.5247,  2.5673],\n",
       "        [-0.7017,  1.3151, -1.0488,  3.1970,  0.9147,  0.6249,  3.3021],\n",
       "        [ 3.3017,  2.3729, -0.7739,  2.3954,  1.3067, -1.3297, -3.2250]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.kdeplot(samps, label=\"DFE\", c='r')\n",
    "sns.kdeplot(prior_samps, label=\"Initial Proposal\", c='g')\n",
    "sns.kdeplot(torch.log10(gamma_samples3), label=\"Moments Proposal\")\n",
    "plt.title(\"Kernel Density Estimation Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "fig.legend([\"DFE\", \"Initial Proposal\", \"Moments Proposal\"], loc=\"lower center\", ncol=3)\n",
    "plt.savefig('ppc_selection_missense_moments.png')\n",
    "plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.kdeplot(mean_samps.cpu().numpy(), label=\"DFE\", c='r')\n",
    "sns.kdeplot(prior_samps, label=\"Initial Proposal\", c='g')\n",
    "sns.kdeplot(torch.log10(gamma_samples2), label=\"Moments Proposal\")\n",
    "plt.title(\"Kernel Density Estimation Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "fig.legend([\"DFE\", \"Initial Proposal\", \"Moments Proposal\"], loc=\"lower center\", ncol=3)\n",
    "plt.savefig('ppc_selection_missense_moments.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.histplot(mean_samps.cpu().numpy())\n",
    "plt.title(\"Histogram Plot Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "plt.savefig('ppc_moments_mis_selectiion_classifier.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.histplot(np.log10(gamma_samples2.cpu().numpy()))\n",
    "plt.title(\"Histogram Plot Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "plt.savefig('ppc_moments_mis_selectiion_classifier_gamma.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.histplot(mean_samps.cpu().numpy())\n",
    "sns.histplot(np.log10(gamma_samples3.cpu().numpy()))\n",
    "plt.title(\"Histogram Plot Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "plt.savefig('ppc_moments_mis_selectiion_classifier_gammas22.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe2= samps.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06 <= |s| < 1e-05: 0.0000000\n",
      "1e-05 <= |s| < 0.0001: 0.0000000\n",
      "0.0001 <= |s| < 0.001: 0.0000000\n",
      "0.001 <= |s| < 0.01: 0.3749375\n",
      "0.01 <= |s| < 0.1: 0.0915938\n",
      "0.1 <= |s| < 1: 0.0851250\n",
      "1 <= |s| < 10000.0: 0.4483437\n",
      "|s| > 10000.0: 0.0000000\n"
     ]
    }
   ],
   "source": [
    "bins = [-6, -5, -4, -3, -2, -1, 0, 4.0]\n",
    "for s0, s1 in zip(bins[:-1], bins[1:]):\n",
    "    the_dat=np.extract((s0 <= dfe2) & (dfe2 < s1), dfe2)\n",
    "    prop = the_dat.shape[0]/obs_samples2.shape[0]\n",
    "    print(f\"{10**s0} <= |s| < {10**s1}: {prop:.7f}\")\n",
    "    if s1 == bins[-1]:\n",
    "        the_dat=np.extract(dfe2 > s1, dfe2)\n",
    "        prop = the_dat.shape[0]/500000.0\n",
    "        print(f\"|s| > {10**s1}: {prop:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss between true and predicted in poisson log-liklihood\n",
    "loss = -1*torch.nn.functional.poisson_nll_loss(torch.log(torch.tensor(mean_predicted2+1)), torch.log(torch.tensor(true_x+1)),log_input=True, full=False, reduction='sum' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predicted = torch.log(torch.tensor(new_predicted_fs).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_target = torch.log(torch.tensor(true_x+1).repeat(log_predicted.shape[0],1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = -1*torch.nn.functional.poisson_nll_loss(log_predicted, log_target, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisson log-liklihood loss -4795.529159783006 vs moments log-liklihood loss for lof -237.79096099886482\n"
     ]
    }
   ],
   "source": [
    "print(\"poisson log-liklihood loss {} vs moments log-liklihood loss for lof {}\".format(loss, moments_loss_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisson log-liklihood loss over batch -47.59079092120809 vs moments log-liklihood loss for lof -237.79096099886482\n"
     ]
    }
   ],
   "source": [
    "print(\"poisson log-liklihood loss over batch {} vs moments log-liklihood loss for lof {}\".format(loss2, moments_loss_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (mean_predicted - true_x)/np.sqrt(mean_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x=x,y=resid, label=\"Residual\")\n",
    "plt.title(\"Poisson Residuals\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "#fig.legend([\"Predicted\", \"Emperical\"], loc=\"lower center\", ncol=2)\n",
    "plt.savefig('resid_lof_32_coefficients_round10.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.5309, 5.0752, 4.4188, 4.0943, 3.6376, 3.1781, 3.1781, 2.6391, 2.5649,\n",
       "         2.7726, 2.7081, 2.7081, 2.7081, 2.7726, 2.3026, 1.0986, 2.3026, 2.3979,\n",
       "         1.6094, 1.9459, 1.3863, 1.6094, 1.3863, 1.9459, 1.9459, 1.7918, 1.3863,\n",
       "         1.6094, 1.0986, 1.0986, 1.6094, 1.7918, 2.0794, 1.6094, 1.0986, 1.6094,\n",
       "         2.0794, 0.6931, 1.0986, 1.3863, 1.3863, 0.6931, 1.0986, 0.6931, 0.0000,\n",
       "         1.0986, 0.6931, 1.0986, 0.6931, 0.6931, 0.0000, 1.0986, 1.0986, 0.6931,\n",
       "         1.0986, 1.0986, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931,\n",
       "         0.0000, 0.6931, 0.6931, 0.6931, 0.0000, 0.0000, 0.6931, 0.6931, 0.0000,\n",
       "         0.6931, 0.6931, 1.3863, 0.6931, 0.6931, 0.0000, 0.6931, 0.6931, 0.0000,\n",
       "         0.0000, 0.0000, 1.3863, 0.6931, 0.6931, 0.6931, 0.0000, 0.6931, 0.6931,\n",
       "         1.0986, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0986,\n",
       "         0.0000, 0.0000, 0.0000, 0.6931, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931,\n",
       "         0.0000, 0.6931, 0.6931, 0.0000, 0.0000, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "         0.0000, 0.0000, 0.6931, 0.0000, 0.0000, 0.0000, 0.0000, 0.6931, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 1.0986, 0.0000, 0.0000, 0.6931, 0.6931, 1.0986, 0.6931, 0.6931,\n",
       "         0.0000, 0.0000, 0.0000, 0.6931, 0.0000, 0.6931, 0.6931, 1.0986, 0.0000,\n",
       "         0.0000, 0.0000, 1.0986, 0.0000, 0.6931, 0.0000, 0.6931]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c011ed9bb21179cd579adf84b3829b40600ff69d12d61fe328e5e2fbe10069c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
