{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.16.0-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/rahul/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi, SNLE, MNLE, SNRE, SNRE_A\n",
    "from sbi.utils.posterior_ensemble import NeuralPosteriorEnsemble\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils import MultipleIndependent\n",
    "from sbi.neural_nets.embedding_nets import PermutationInvariantEmbedding, FCEmbedding\n",
    "from sbi.utils.user_input_checks import process_prior, process_simulator\n",
    "from sbi.utils import get_density_thresholder, RestrictedPrior\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "import numpy as np\n",
    "import moments\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import atexit\n",
    "import torch.nn.functional as F\n",
    "import subprocess\n",
    "import sparselinear as sl\n",
    "from sortedcontainers import SortedDict\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "import re\n",
    "from monarch_linear import MonarchLinear\n",
    "import pdb\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR) # See: https://github.com/matplotlib/matplotlib/issues/14523\n",
    "from collections import defaultdict\n",
    "from sbi.analysis import pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportanceSamplingEstimator(sample, threshold, target=None, num_particles=None):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sample (_type_): _description_\n",
    "        target (_type_): _description_\n",
    "        threshold (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    cdftest = target.q.transforms\n",
    "    low_samples = sample - .0025\n",
    "    high_samples = sample + .0025 \n",
    "    num_particles = 1000 if num_particles is None else num_particles\n",
    "    if target is not None:\n",
    "        with torch.no_grad():\n",
    "            for transform in cdftest[::-1]:\n",
    "                value = transform.inv(high_samples)\n",
    "            if target.q._validate_args:\n",
    "                target.q.base_dist._validate_sample(value)\n",
    "            value = target.q.base_dist.base_dist.cdf(value)\n",
    "            #value = target.q._monotonize_cdf(value)\n",
    "        with torch.no_grad():\n",
    "            for transform in cdftest[::-1]:\n",
    "                value2 = transform.inv(low_samples)\n",
    "            if target.q._validate_args:\n",
    "                target.q.base_dist._validate_sample(value2)\n",
    "            value2 = target.q.base_dist.base_dist.cdf(value2)\n",
    "            #value2 = target.q.base_dist._monotonize_cdf(value2)\n",
    "        \n",
    "        return value - value2\n",
    "    else:\n",
    "        sample_low = sample-threshold\n",
    "        sample_high = sample+threshold\n",
    "        proposal = torch.distributions.uniform.Uniform(sample_low, sample_high)\n",
    "        prop_samps = proposal.sample((num_particles,))\n",
    "        target_logprobs = target.log_prob(prop_samps)\n",
    "        proposal_logprobs = proposal.log_prob(prop_samps)\n",
    "        log_importance_weights = target_logprobs - proposal_logprobs\n",
    "\n",
    "    ret = torch.sum(torch.exp(log_importance_weights))/num_particles\n",
    "\n",
    "\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moments_sim_data(prior: float) -> torch.float32:\n",
    "    \n",
    "    global sample_size\n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    #s_prior, weights = prior[:6], prior[6:]\n",
    "    #s_prior, weights = prior[:5], prior[5:]\n",
    "    s_prior, p_misid, weights = prior[:7], prior[7], prior[7:]\n",
    "    fs_aggregate = None\n",
    "    gammas = s_prior.cpu().numpy().squeeze()\n",
    "    weights = weights.cpu().numpy().squeeze()\n",
    "    p_misid = p_misid.cpu().numpy()\n",
    "    for j, (gamma, weight) in enumerate(zip(gammas, weights)):\n",
    "        while rerun:\n",
    "            ns_sim = 2 * ns_sim\n",
    "            fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "            fs = moments.Spectrum(fs)\n",
    "            fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "            nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "                np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "            fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "            if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "                # large gamma-values can require large sample sizes for stability\n",
    "                rerun = True\n",
    "            else:\n",
    "                rerun = False\n",
    "        if j == 0:\n",
    "            fs_aggregate = fs.project([projected_sample_size]).compressed()*theta_mis * weight\n",
    "            fs_aggregate = (1 - p_misid)*fs_aggregate + p_misid * fs_aggregate[::-1]\n",
    "        else:\n",
    "            fs_aggregate += fs.project([projected_sample_size]).compressed()*theta_mis * weight\n",
    "            fs_aggregate = (1 - p_misid)*fs_aggregate + p_misid * fs_aggregate[::-1]\n",
    "\n",
    "    fs_aggregate = torch.tensor(fs_aggregate).type(torch.float32) \n",
    "    return fs_aggregate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_moments_sim_data2(prior: float) -> torch.float32:\n",
    "    \n",
    "    global sample_size\n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    #s_prior, weights = prior[:6], prior[6:]\n",
    "    #s_prior, weights = prior[:5], prior[5:]\n",
    "    #s_prior, p_misid, weights = prior[:7], prior[7], prior[7:]\n",
    "    fs_aggregate = None\n",
    "    p_misid = 0.0137\n",
    "    gammas = -1*10**(prior.cpu().numpy().squeeze())\n",
    "   \n",
    "    for j, gamma in enumerate(gammas):\n",
    "        while rerun:\n",
    "            ns_sim = 2 * ns_sim\n",
    "            fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "            fs = moments.Spectrum(fs)\n",
    "            fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "            nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "                np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "            fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "            if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "                # large gamma-values can require large sample sizes for stability\n",
    "                rerun = True\n",
    "            else:\n",
    "                rerun = False\n",
    "        if j == 0:\n",
    "            fs2 = fs.project([projected_sample_size]).compressed()*theta_mis\n",
    "            fs2 = (1 - p_misid) * fs2 + p_misid * fs2[::-1]\n",
    "            fs_aggregate = fs2\n",
    "        else:\n",
    "            fs2 = fs.project([projected_sample_size]).compressed()*theta_mis\n",
    "            fs2 = (1 - p_misid) * fs2 + p_misid * fs2[::-1]\n",
    "            fs_aggregate += fs2\n",
    "            del fs2\n",
    "            \n",
    "    fs_aggregate /= gammas.shape[0]\n",
    "    fs_aggregate = torch.poisson(torch.tensor(fs_aggregate)).type(torch.float32) \n",
    "    return fs_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_momments(prior: float, sample_size) -> torch.float32:\n",
    "    \n",
    "    opt_params = [2.21531687, 5.29769918, 0.55450117, 0.04088086]\n",
    "    theta_mis = 15583.437265450002\n",
    "    theta_lof = 1164.3148344084038\n",
    "    rerun = True\n",
    "    ns_sim = 100\n",
    "    h=0.5\n",
    "    projected_sample_size = sample_size*2\n",
    "    gammas = -1*10**(prior)\n",
    "\n",
    "    while rerun:\n",
    "        ns_sim = 2 * ns_sim\n",
    "        fs = moments.LinearSystem_1D.steady_state_1D(ns_sim, gamma=gamma, h=h)\n",
    "        fs = moments.Spectrum(fs)\n",
    "        fs.integrate([opt_params[0]], opt_params[2], gamma=gamma, h=h)\n",
    "        nu_func = lambda t: [opt_params[0] * np.exp(\n",
    "            np.log(opt_params[1] / opt_params[0]) * t / opt_params[3])]\n",
    "        fs.integrate(nu_func, opt_params[3], gamma=gamma, h=h)\n",
    "        if abs(np.max(fs)) > 10 or np.any(np.isnan(fs)):\n",
    "            # large gamma-values can require large sample sizes for stability\n",
    "            rerun = True\n",
    "        else:\n",
    "            rerun = False\n",
    "        fs2 = fs.project([projected_sample_size]).compressed()*theta_mis\n",
    "\n",
    "\n",
    "   \n",
    "    return fs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.poisson(torch.abs(torch.tensor(generate_momments(-3, 85)*15583)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.7659e+08, 2.6023e+08, 1.5035e+08, 1.0196e+08, 7.4956e+07, 5.7785e+07,\n",
       "        4.5954e+07, 3.7372e+07, 3.0901e+07, 2.5886e+07, 2.1907e+07, 1.8718e+07,\n",
       "        1.6108e+07, 1.3943e+07, 1.2142e+07, 1.0625e+07, 9.3428e+06, 8.2343e+06,\n",
       "        7.2927e+06, 6.4832e+06, 5.7749e+06, 5.1637e+06, 4.6275e+06, 4.1499e+06,\n",
       "        3.7357e+06, 3.3714e+06, 3.0467e+06, 2.7600e+06, 2.5021e+06, 2.2734e+06,\n",
       "        2.0701e+06, 1.8896e+06, 1.7251e+06, 1.5772e+06, 1.4456e+06, 1.3229e+06,\n",
       "        1.2177e+06, 1.1203e+06, 1.0316e+06, 9.5172e+05, 8.7733e+05, 8.1359e+05,\n",
       "        7.5097e+05, 6.9594e+05, 6.4496e+05, 5.9884e+05, 5.5723e+05, 5.1825e+05,\n",
       "        4.8411e+05, 4.5120e+05, 4.2000e+05, 3.9252e+05, 3.6800e+05, 3.4562e+05,\n",
       "        3.2281e+05, 3.0260e+05, 2.8444e+05, 2.6742e+05, 2.5100e+05, 2.3663e+05,\n",
       "        2.2238e+05, 2.1040e+05, 1.9849e+05, 1.8694e+05, 1.7688e+05, 1.6800e+05,\n",
       "        1.5847e+05, 1.5046e+05, 1.4256e+05, 1.3514e+05, 1.2841e+05, 1.2195e+05,\n",
       "        1.1569e+05, 1.1075e+05, 1.0537e+05, 1.0101e+05, 9.5392e+04, 9.1133e+04,\n",
       "        8.7246e+04, 8.2670e+04, 7.9907e+04, 7.6014e+04, 7.2891e+04, 6.9938e+04,\n",
       "        6.7077e+04, 6.4568e+04, 6.1942e+04, 5.8951e+04, 5.6493e+04, 5.3834e+04,\n",
       "        5.2170e+04, 5.0134e+04, 4.8418e+04, 4.6636e+04, 4.5331e+04, 4.3274e+04,\n",
       "        4.1736e+04, 4.0172e+04, 3.8541e+04, 3.7539e+04, 3.6194e+04, 3.4853e+04,\n",
       "        3.3999e+04, 3.2403e+04, 3.1472e+04, 3.0631e+04, 2.9636e+04, 2.8959e+04,\n",
       "        2.7791e+04, 2.6626e+04, 2.6248e+04, 2.5448e+04, 2.4492e+04, 2.3995e+04,\n",
       "        2.3019e+04, 2.2552e+04, 2.1861e+04, 2.1371e+04, 2.0536e+04, 1.9860e+04,\n",
       "        1.9582e+04, 1.8936e+04, 1.8635e+04, 1.8089e+04, 1.7588e+04, 1.6835e+04,\n",
       "        1.6380e+04, 1.6280e+04, 1.5787e+04, 1.5390e+04, 1.5118e+04, 1.4685e+04,\n",
       "        1.4297e+04, 1.3816e+04, 1.3730e+04, 1.3498e+04, 1.2951e+04, 1.2695e+04,\n",
       "        1.2419e+04, 1.2174e+04, 1.1850e+04, 1.1368e+04, 1.1361e+04, 1.1267e+04,\n",
       "        1.0831e+04, 1.0679e+04, 1.0175e+04, 1.0048e+04, 9.9840e+03, 9.7520e+03,\n",
       "        9.6600e+03, 9.2420e+03, 9.1860e+03, 9.0450e+03, 8.5740e+03, 8.7850e+03,\n",
       "        8.4900e+03, 8.2970e+03, 8.0910e+03, 8.0740e+03, 7.8660e+03, 7.6350e+03,\n",
       "        7.6830e+03, 7.3020e+03, 7.4830e+03, 7.2050e+03, 6.9060e+03, 6.9410e+03,\n",
       "        6.8450e+03], dtype=torch.float64)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.poisson(torch.abs(torch.tensor(generate_momments(-1, 85)*15583)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4567e+04, 1.7949e+04, 1.0958e+04, 7.6070e+03, 5.8640e+03, 4.6710e+03,\n",
       "        4.0230e+03, 3.3090e+03, 2.8310e+03, 2.5110e+03, 2.1770e+03, 1.9510e+03,\n",
       "        1.7350e+03, 1.6050e+03, 1.4240e+03, 1.2950e+03, 1.1680e+03, 1.1090e+03,\n",
       "        1.0190e+03, 9.4700e+02, 8.8800e+02, 8.1100e+02, 7.7900e+02, 7.1000e+02,\n",
       "        6.5900e+02, 6.1300e+02, 6.3600e+02, 5.3400e+02, 5.0900e+02, 5.0100e+02,\n",
       "        4.5300e+02, 4.4300e+02, 4.0300e+02, 3.8200e+02, 3.4400e+02, 3.6200e+02,\n",
       "        3.4300e+02, 3.1700e+02, 3.4100e+02, 2.8800e+02, 3.1900e+02, 2.6900e+02,\n",
       "        2.5500e+02, 2.6200e+02, 2.5100e+02, 2.1800e+02, 2.3800e+02, 2.3300e+02,\n",
       "        2.0900e+02, 1.9300e+02, 1.9700e+02, 1.7000e+02, 1.9100e+02, 1.6700e+02,\n",
       "        1.6700e+02, 1.5900e+02, 1.5600e+02, 1.4600e+02, 1.5300e+02, 1.0100e+02,\n",
       "        1.1700e+02, 1.4200e+02, 1.1900e+02, 1.4600e+02, 1.2800e+02, 1.3000e+02,\n",
       "        1.2000e+02, 1.2400e+02, 1.1000e+02, 1.3300e+02, 1.0600e+02, 1.0400e+02,\n",
       "        1.1000e+02, 9.8000e+01, 1.0600e+02, 8.5000e+01, 7.3000e+01, 9.8000e+01,\n",
       "        8.6000e+01, 9.7000e+01, 8.8000e+01, 7.1000e+01, 8.6000e+01, 8.0000e+01,\n",
       "        7.8000e+01, 8.2000e+01, 6.4000e+01, 7.0000e+01, 6.1000e+01, 6.3000e+01,\n",
       "        6.8000e+01, 5.4000e+01, 8.3000e+01, 5.8000e+01, 7.6000e+01, 6.9000e+01,\n",
       "        4.2000e+01, 5.9000e+01, 7.2000e+01, 5.3000e+01, 6.3000e+01, 6.1000e+01,\n",
       "        5.8000e+01, 5.9000e+01, 6.7000e+01, 4.9000e+01, 6.4000e+01, 6.2000e+01,\n",
       "        5.9000e+01, 4.7000e+01, 5.9000e+01, 4.5000e+01, 5.6000e+01, 5.9000e+01,\n",
       "        3.7000e+01, 5.4000e+01, 3.5000e+01, 6.5000e+01, 3.9000e+01, 4.9000e+01,\n",
       "        3.6000e+01, 4.3000e+01, 5.2000e+01, 3.9000e+01, 4.0000e+01, 3.4000e+01,\n",
       "        3.5000e+01, 3.4000e+01, 2.9000e+01, 4.1000e+01, 3.4000e+01, 3.7000e+01,\n",
       "        2.9000e+01, 2.5000e+01, 3.2000e+01, 2.8000e+01, 2.6000e+01, 2.9000e+01,\n",
       "        3.7000e+01, 2.0000e+01, 2.6000e+01, 2.9000e+01, 2.7000e+01, 2.6000e+01,\n",
       "        3.5000e+01, 3.2000e+01, 3.5000e+01, 2.1000e+01, 2.5000e+01, 2.6000e+01,\n",
       "        2.9000e+01, 2.3000e+01, 2.1000e+01, 1.7000e+01, 2.7000e+01, 2.9000e+01,\n",
       "        2.5000e+01, 2.2000e+01, 2.0000e+01, 2.7000e+01, 2.5000e+01, 2.2000e+01,\n",
       "        1.7000e+01, 1.6000e+01, 2.3000e+01, 1.8000e+01, 2.9000e+01, 1.9000e+01,\n",
       "        2.0000e+01], dtype=torch.float64)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_posterior = torch.load('Experiments/saved_posteriors_msl_mcf_6_and_psmid_params_2023-04-06_14-14/posterior_observed_round_15.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_x = np.load('emperical_missense_sfs_msl.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 11.90 GiB total capacity; 296.54 MiB already allocated; 78.12 MiB free; 382.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accept_reject_fn \u001b[39m=\u001b[39m get_density_thresholder(last_posterior, quantile\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m, num_samples_to_estimate_support\u001b[39m=\u001b[39;49m\u001b[39m30000\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m proposal \u001b[39m=\u001b[39m RestrictedPrior(last_posterior\u001b[39m.\u001b[39m_prior, accept_reject_fn, last_posterior, sample_with\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msir\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/sbi/utils/restriction_estimator.py:544\u001b[0m, in \u001b[0;36mget_density_thresholder\u001b[0;34m(dist, quantile, num_samples_to_estimate_support)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_density_thresholder\u001b[39m(\n\u001b[1;32m    520\u001b[0m     dist: Any,\n\u001b[1;32m    521\u001b[0m     quantile: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m,\n\u001b[1;32m    522\u001b[0m     num_samples_to_estimate_support: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1_000_000\u001b[39m,\n\u001b[1;32m    523\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Callable:\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\"\"Returns function that thresholds a density at a particular `1-quantile`.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \n\u001b[1;32m    526\u001b[0m \u001b[39m    Reference:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39m        of the `dist`.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m     samples \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39;49msample((num_samples_to_estimate_support,))\n\u001b[1;32m    545\u001b[0m     log_probs \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mlog_prob(samples)\n\u001b[1;32m    546\u001b[0m     sorted_log_probs, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msort(log_probs)\n",
      "File \u001b[0;32m~/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/sbi/inference/posteriors/vi_posterior.py:297\u001b[0m, in \u001b[0;36mVIPosterior.sample\u001b[0;34m(self, sample_shape, x, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trained_on \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (x \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trained_on)\u001b[39m.\u001b[39mall():\n\u001b[1;32m    293\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    294\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe variational posterior was not fit on the specified `default_x` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m. Please train using `posterior.train()`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m     )\n\u001b[0;32m--> 297\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq\u001b[39m.\u001b[39;49msample(torch\u001b[39m.\u001b[39;49mSize(sample_shape))\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m samples\u001b[39m.\u001b[39mreshape((\u001b[39m*\u001b[39msample_shape, samples\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/transformed_distribution.py:118\u001b[0m, in \u001b[0;36mTransformedDistribution.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_dist\u001b[39m.\u001b[39msample(sample_shape)\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m--> 118\u001b[0m     x \u001b[39m=\u001b[39m transform(x)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/transforms.py:155\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m x_old:\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m y_old\n\u001b[0;32m--> 155\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(x)\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_x_y \u001b[39m=\u001b[39m x, y\n\u001b[1;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pyro/distributions/transforms/spline_autoregressive.py:97\u001b[0m, in \u001b[0;36mSplineAutoregressive._call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m:param x: the input into the bijection\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m:type x: torch.Tensor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mthe base distribution (or the output of a previous transform)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m spline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspline\u001b[39m.\u001b[39mcondition(x)\n\u001b[0;32m---> 97\u001b[0m y \u001b[39m=\u001b[39m spline(x)\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_log_detJ \u001b[39m=\u001b[39m spline\u001b[39m.\u001b[39m_cache_log_detJ\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/transforms.py:155\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m x_old:\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m y_old\n\u001b[0;32m--> 155\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(x)\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_x_y \u001b[39m=\u001b[39m x, y\n\u001b[1;32m    157\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pyro/distributions/transforms/spline.py:320\u001b[0m, in \u001b[0;36mConditionedSpline._call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 320\u001b[0m     y, log_detJ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspline_op(x)\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_log_detJ \u001b[39m=\u001b[39m log_detJ\n\u001b[1;32m    322\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pyro/distributions/transforms/spline.py:350\u001b[0m, in \u001b[0;36mConditionedSpline.spline_op\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspline_op\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    349\u001b[0m     w, h, d, l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params() \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params\n\u001b[0;32m--> 350\u001b[0m     y, log_detJ \u001b[39m=\u001b[39m _monotonic_rational_spline(\n\u001b[1;32m    351\u001b[0m         x, w, h, d, l, bound\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     \u001b[39mreturn\u001b[39;00m y, log_detJ\n",
      "File \u001b[0;32m~/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pyro/distributions/transforms/spline.py:148\u001b[0m, in \u001b[0;36m_monotonic_rational_spline\u001b[0;34m(inputs, widths, heights, derivatives, lambdas, inverse, bound, min_bin_width, min_bin_height, min_derivative, min_lambda, eps)\u001b[0m\n\u001b[1;32m    142\u001b[0m derivatives \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    143\u001b[0m     derivatives, pad\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m min_derivative\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    146\u001b[0m \u001b[39m# Get the index of the bin that each input is in\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39m# bin_idx ~ (batch_dim, input_dim, 1)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m bin_idx \u001b[39m=\u001b[39m _searchsorted(\n\u001b[1;32m    149\u001b[0m     cumheights \u001b[39m+\u001b[39;49m eps \u001b[39mif\u001b[39;49;00m inverse \u001b[39melse\u001b[39;49;00m cumwidths \u001b[39m+\u001b[39;49m eps, inputs\n\u001b[1;32m    150\u001b[0m )\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[39m# Select the value for the relevant bin for the variables used in the main calculation\u001b[39;00m\n\u001b[1;32m    153\u001b[0m input_widths \u001b[39m=\u001b[39m _select_bins(widths, bin_idx)\n",
      "File \u001b[0;32m~/PopGen/SimulationSFS/SFS/lib/python3.10/site-packages/pyro/distributions/transforms/spline.py:34\u001b[0m, in \u001b[0;36m_searchsorted\u001b[0;34m(sorted_sequence, values)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_searchsorted\u001b[39m(sorted_sequence, values):\n\u001b[1;32m     28\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    Searches for which bin an input belongs to (in a way that is parallelizable and\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    amenable to autodiff)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[39m    TODO: Replace with torch.searchsorted once it is released\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49msum(values[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m] \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m sorted_sequence, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 11.90 GiB total capacity; 296.54 MiB already allocated; 78.12 MiB free; 382.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "accept_reject_fn = get_density_thresholder(last_posterior, quantile=1e-5, num_samples_to_estimate_support=30000)\n",
    "proposal = RestrictedPrior(last_posterior._prior, accept_reject_fn, last_posterior, sample_with=\"sir\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fs=[]\n",
    "predicted_fs2=[]\n",
    "sample_size = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples = proposal.sample((2000,), oversampling_factor=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4829, -2.1209, -2.2635, -2.4465, -2.9487, -2.0098, -2.0398, -1.7250,\n",
       "        -2.0853, -3.0351, -2.1298, -2.4436, -2.1012, -2.0673, -2.4945, -2.3914,\n",
       "        -2.1178, -1.9946, -2.1168, -1.8758, -2.0457, -2.3790, -2.3992, -1.8551,\n",
       "        -2.3266, -1.9339, -1.9661, -2.3549, -1.9196, -2.6118, -2.2146, -2.1941,\n",
       "        -2.6707, -2.2191, -1.5458, -2.0106, -2.1676, -1.8533, -2.0701, -2.4434],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(obs_samples,dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80000])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.ioff()\n",
    "obs_samples2 = obs_samples.view(-1)\n",
    "obs_samples2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "samps = obs_samples2.cpu().numpy()\n",
    "sns.histplot(samps)\n",
    "plt.savefig('posterior_selection_coef.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sampled selection coefficients: torch.Size([2000, 20])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of sampled selection coefficients: {}\".format(obs_samples.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs_sample in obs_samples:\n",
    "    \n",
    "    fs = generate_moments_sim_data2(obs_sample)\n",
    "    predicted_fs.append(fs.unsqueeze(0).cpu().numpy())\n",
    "    predicted_fs2.append(np.log10(fs[1:169:10].unsqueeze(0).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frequency spectrum containing all bins (2000, 169)\n"
     ]
    }
   ],
   "source": [
    "new_predicted_fs = np.asarray(predicted_fs).squeeze(1)\n",
    "print(\"Shape of frequency spectrum containing all bins {}\".format(new_predicted_fs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_true_x = np.log10(true_x[1:169:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predicted_fs2 = np.log10(new_predicted_fs[:,1:169:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of indices (bin) that were chosen to plot 17\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(1,169,10)\n",
    "print(\"Shape of indices (bin) that were chosen to plot {}\".format(idx.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  11,  21,  31,  41,  51,  61,  71,  81,  91, 101, 111, 121,\n",
       "       131, 141, 151, 161])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many subplots to create per bin: (17,)\n",
      "Shape of predicted SFS: (2000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"How many subplots to create per bin: {}\".format(smaller_true_x.shape))\n",
    "print(\"Shape of predicted SFS: {}\".format(new_predicted_fs2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "x_points = np.arange(1, smaller_true_x.shape[0])\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.6)\n",
    "for i in range(1, 25):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    #sns.scatterplot(ax=ax, x=predicted_fs2[:,i-1], y=x_points[i-1])\n",
    "    sns.histplot(np.log10(new_predicted_fs[:,i-1]))\n",
    "    plt.axvline(x=np.mean(np.log10(new_predicted_fs[:,i-1])), color='m', label=\"mean\")\n",
    "    plt.axvline(x=np.median(np.log10(new_predicted_fs[:,i-1])), color='k', label=\"median\")\n",
    "    ax.axline((np.log10(true_x[i-1]), 1), (np.log10(true_x[i-1]),100), marker='+', c='r', label=\"Emperical SFS\")\n",
    "    #true_diff_predicted = 10**(np.mean(new_predicted_fs2[:,i-1])) - 10**(smaller_true_x[i-1])\n",
    "    #plt.text(x=((np.mean(new_predicted_fs2[:,i-1])+smaller_true_x[i-1])/2), y=50, s=\"{:.2f}.\".format(true_diff_predicted))\n",
    "    #ax.plot(smaller_true_x[i-1], x_points[i-1], markersize=7.0, c='r', marker='+')\n",
    "    #plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(smaller_true_x[i-1], idx[i-1] ))\n",
    "    plt.title(\"Emperical SFS: 10^{:.3f} at bin: {}\".format(np.log10(true_x[i-1]),i))\n",
    "fig.legend([\"mean\", \"median\", \"Emperical SFS\"], loc=\"lower center\", ncol=4)\n",
    "plt.savefig('ppc_check_hist_averaged_samples_is_40_different_bins.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predicted = np.mean(new_predicted_fs[:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44606.355 , 18036.723 , 10953.882 ,  7805.9033,  6024.93  ,\n",
       "        4882.157 ,  4076.6145,  3481.797 ,  3023.556 ,  2660.1836,\n",
       "        2366.924 ,  2124.216 ,  1919.974 ,  1746.943 ,  1598.359 ,\n",
       "        1471.364 ,  1359.3855,  1260.101 ,  1171.34  ,  1093.393 ,\n",
       "        1024.5284,   961.585 ,   905.8385,   853.722 ,   806.275 ,\n",
       "         766.1625,   725.4145,   690.0025,   656.746 ,   627.951 ,\n",
       "         600.7235,   572.4925,   549.3955,   527.0005,   506.0025,\n",
       "         485.702 ,   468.388 ,   450.8835,   435.0095,   420.4755,\n",
       "         406.683 ,   393.7765,   381.103 ,   368.8925,   356.7455,\n",
       "         347.734 ,   336.9785,   327.3815,   318.447 ,   309.545 ,\n",
       "         301.9875,   294.3455,   287.2055,   279.658 ,   273.996 ,\n",
       "         266.9535,   260.7385,   256.083 ,   249.4475,   244.08  ,\n",
       "         238.929 ,   234.353 ,   229.043 ,   224.8635,   220.698 ,\n",
       "         216.174 ,   212.639 ,   208.8525,   205.0715,   201.8095,\n",
       "         197.828 ,   194.6575,   191.204 ,   188.152 ,   185.1035,\n",
       "         182.882 ,   180.0445,   177.298 ,   174.0325,   172.3425,\n",
       "         169.628 ,   166.8665,   165.1235,   162.126 ,   160.1615,\n",
       "         157.454 ,   156.1815,   153.9195,   152.0775,   150.238 ,\n",
       "         148.5375,   146.392 ,   145.0835,   143.2985,   141.605 ,\n",
       "         140.0415,   138.2025,   136.5375,   135.1135,   133.935 ,\n",
       "         132.3815,   131.024 ,   129.6985,   127.6245,   127.094 ,\n",
       "         126.0775,   124.3205,   123.64  ,   122.1575,   121.183 ,\n",
       "         119.6615,   118.8175,   118.2555,   116.5575,   116.023 ,\n",
       "         115.0255,   113.911 ,   112.8635,   112.567 ,   111.182 ,\n",
       "         110.414 ,   109.376 ,   108.659 ,   107.8595,   107.    ,\n",
       "         106.1635,   105.3935,   105.01  ,   104.4095,   103.6645,\n",
       "         102.6675,   102.18  ,   101.413 ,   101.1555,   100.8685,\n",
       "         100.333 ,   100.142 ,    99.7045,    99.0145,    98.557 ,\n",
       "          98.524 ,    98.6765,    98.3385,    98.1725,    97.4595,\n",
       "          97.9135,    97.8095,    98.304 ,    98.3645,    98.5145,\n",
       "          98.9635,    99.8185,   100.5835,   101.372 ,   102.517 ,\n",
       "         104.3735,   105.8455,   108.535 ,   110.8455,   114.742 ,\n",
       "         119.3875,   124.6655,   132.8195,   143.356 ,   158.65  ,\n",
       "         182.73  ,   226.201 ,   323.6595,   691.8445], dtype=float32)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,mean_predicted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "sns.scatterplot(x=x,y=np.log10(mean_predicted))\n",
    "sns.scatterplot(x=x,y=np.log10(true_x))\n",
    "plt.title(\"Mean of posterior predicted SFS vs True SFS\")\n",
    "plt.ylabel(\"Log Scaled Allele Frequency\")\n",
    "plt.xlabel(\"Frequency Bin\")\n",
    "plt.savefig('ppc_scatter.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_samples2 = obs_samples.view(-1)\n",
    "obs_samples2.shape\n",
    "samps = obs_samples2.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samps =last_posterior._prior.sample((2000,)).view(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.kdeplot(samps)\n",
    "sns.kdeplot(prior_samps)\n",
    "plt.title(\"Kernel Density Estimation Inferred Scaled Selection\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Log of Absolute Scaled Selection Coefficient\")\n",
    "plt.savefig('ppc_kde_selection.png')\n",
    "plt.close()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c011ed9bb21179cd579adf84b3829b40600ff69d12d61fe328e5e2fbe10069c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
